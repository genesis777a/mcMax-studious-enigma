# AI SDK - Provider Implementation Utilities
/
BERT testing
{pretrained mistral AI model}

Developed by Google in 2018, BERT (Bidirectional Encoder Representations from Transformers) introduced a revolutionary idea â€” bidirectional attention. Unlike older models that read text from left-to-right, BERT reads in both directions simultaneously. This allowed it to understand full sentence meaning, not just local context.

  - Architecture: Transformer Encoder-only
  - Architecture Type: Purely Encoder-based (no decoder used)
  - Training Tasks:
  - Masked Language Modeling (predict missing words)
  - Next Sentence Prediction (check if two sentences follow logically)
  - Use Cases: Sentiment analysis, Q&A, sentence classification
  - Model Sizes:
    BERT Base: 110M parameters
    BERT Large: 340M parameters
